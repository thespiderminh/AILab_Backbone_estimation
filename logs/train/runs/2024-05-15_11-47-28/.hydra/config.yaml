task_name: train
tags:
- mnist
- simple_dense_net
train: true
test: true
ckpt_path: null
seed: 12345
data:
  _target_: src.data.lsp_master_datamodule.DataModule
  batch_size: 32
  train_val_test_split:
  - 0.6
  - 0.2
  - 0.2
  num_workers: 2
  pin_memory: false
model:
  _target_: src.models.lsp_model_module.ModelModule
  optimizer:
    _target_: torch.optim.Adam
    _partial_: true
    lr: 0.002
    weight_decay: 0
  scheduler:
    _target_: torch.optim.lr_scheduler.ReduceLROnPlateau
    _partial_: true
    mode: min
    factor: 0.1
    patience: 10
  net:
    _target_: src.models.components.omnipose.OmniPose
    cfg:
      AUTO_RESUME: true
      CUDNN:
        BENCHMARK: true
        DETERMINISTIC: false
        ENABLED: true
      DATA_DIR: ''
      GPUS: (0,)
      OUTPUT_DIR: output
      LOG_DIR: log
      WORKERS: 24
      PRINT_FREQ: 100
      DATASET:
        COLOR_RGB: true
        DATASET: mpii
        DATA_FORMAT: jpg
        FLIP: true
        NUM_JOINTS_HALF_BODY: 8
        PROB_HALF_BODY: -1.0
        ROOT: data/mpii/
        ROT_FACTOR: 30
        SCALE_FACTOR: 0.25
        TEST_SET: valid
        TRAIN_SET: train
      MODEL:
        INIT_WEIGHTS: true
        NAME: omnipose
        NUM_JOINTS: 16
        PRETRAINED: weights/mpii/OmniPose_w48/model_best
        TARGET_TYPE: gaussian
        IMAGE_SIZE:
        - 256
        - 256
        HEATMAP_SIZE:
        - 64
        - 64
        SIGMA: 2
        EXTRA:
          PRETRAINED_LAYERS:
          - conv1
          - bn1
          - conv2
          - bn2
          - layer1
          - transition1
          - stage2
          - transition2
          - stage3
          - transition3
          - stage4
          FINAL_CONV_KERNEL: 1
          STAGE2:
            NUM_MODULES: 1
            NUM_BRANCHES: 2
            BLOCK: BASIC
            NUM_BLOCKS:
            - 4
            - 4
            NUM_CHANNELS:
            - 48
            - 96
            FUSE_METHOD: SUM
          STAGE3:
            NUM_MODULES: 4
            NUM_BRANCHES: 3
            BLOCK: BASIC
            NUM_BLOCKS:
            - 4
            - 4
            - 4
            NUM_CHANNELS:
            - 48
            - 96
            - 192
            FUSE_METHOD: SUM
          STAGE4:
            NUM_MODULES: 3
            NUM_BRANCHES: 4
            BLOCK: BASIC
            NUM_BLOCKS:
            - 4
            - 4
            - 4
            - 4
            NUM_CHANNELS:
            - 48
            - 96
            - 192
            - 384
            FUSE_METHOD: SUM
          STAGE5:
            NUM_MODULES: 3
            NUM_BRANCHES: 5
            BLOCK: BASIC
            NUM_BLOCKS:
            - 4
            - 4
            - 4
            - 4
            - 4
            NUM_CHANNELS:
            - 48
            - 96
            - 192
            - 384
            - 768
            FUSE_METHOD: SUM
      LOSS:
        USE_TARGET_WEIGHT: true
      TRAIN:
        BATCH_SIZE_PER_GPU: 16
        SHUFFLE: true
        BEGIN_EPOCH: 0
        END_EPOCH: 210
        OPTIMIZER: adam
        LR: 0.0001
        LR_FACTOR: 0.1
        LR_STEP:
        - 170
        - 200
        WD: 0.0001
        GAMMA1: 0.99
        GAMMA2: 0.0
        MOMENTUM: 0.9
        NESTEROV: false
      TEST:
        BATCH_SIZE_PER_GPU: 16
        MODEL_FILE: ''
        FLIP_TEST: true
        POST_PROCESS: true
        BLUR_KERNEL: 11
      DEBUG:
        DEBUG: true
        SAVE_BATCH_IMAGES_GT: true
        SAVE_BATCH_IMAGES_PRED: true
        SAVE_HEATMAPS_GT: true
        SAVE_HEATMAPS_PRED: true
  compile: false
callbacks:
  model_checkpoint:
    _target_: lightning.pytorch.callbacks.ModelCheckpoint
    dirpath: ${paths.output_dir}/checkpoints
    filename: epoch_{epoch:03d}
    monitor: val/loss
    verbose: false
    save_last: true
    save_top_k: 1
    mode: min
    auto_insert_metric_name: false
    save_weights_only: false
    every_n_train_steps: null
    train_time_interval: null
    every_n_epochs: null
    save_on_train_epoch_end: null
  early_stopping:
    _target_: lightning.pytorch.callbacks.EarlyStopping
    monitor: val/loss
    min_delta: 0.0
    patience: 100
    verbose: false
    mode: min
    strict: true
    check_finite: true
    stopping_threshold: null
    divergence_threshold: null
    check_on_train_epoch_end: null
  model_summary:
    _target_: lightning.pytorch.callbacks.RichModelSummary
    max_depth: -1
  rich_progress_bar:
    _target_: lightning.pytorch.callbacks.RichProgressBar
trainer:
  _target_: lightning.pytorch.trainer.Trainer
  default_root_dir: ${paths.output_dir}
  min_epochs: 10
  max_epochs: 10
  accelerator: gpu
  devices: 1
  check_val_every_n_epoch: 1
  deterministic: false
  gradient_clip_val: 0.5
paths:
  root_dir: ${oc.env:PROJECT_ROOT}
  data_dir: ${paths.root_dir}/data/
  log_dir: ${paths.root_dir}/logs/
  output_dir: ${hydra:runtime.output_dir}
  work_dir: ${hydra:runtime.cwd}
extras:
  ignore_warnings: false
  enforce_tags: true
  print_config: true
logger:
  wandb:
    tags: ${tags}
    group: mnist
  aim:
    experiment: mnist
